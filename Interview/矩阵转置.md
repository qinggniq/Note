# 512 * 512 和 513 * 513 矩阵转置谁更快

## 问题背景

给定两个整型矩阵，其中一个为 512 * 512 ，另一个大小为513 * 513，对两个矩阵分别进行矩阵转置（也就是矩阵左下角的元素与矩阵右上角的元素互换），那个矩阵花费的时间会更少？[源问题](https://stackoverflow.com/questions/11413855/why-is-transposing-a-matrix-of-512x512-much-slower-than-transposing-a-matrix-of)

```c
#define SAMPLES 1000
#define MATSIZE 512

#include <time.h>
#include <iostream>
int mat[MATSIZE][MATSIZE];

void transpose()
{
   for ( int i = 0 ; i < MATSIZE ; i++ )
   for ( int j = 0 ; j < MATSIZE ; j++ )
   {
       int aux = mat[i][j];
       mat[i][j] = mat[j][i];
       mat[j][i] = aux;
   }
}

int main()
{
   //initialize matrix
   for ( int i = 0 ; i < MATSIZE ; i++ )
   for ( int j = 0 ; j < MATSIZE ; j++ )
       mat[i][j] = i+j;

   int t = clock();
   for ( int i = 0 ; i < SAMPLES ; i++ )
       transpose();
   int elapsed = clock() - t;

   std::cout << "Average for a matrix of " << MATSIZE << ": " << elapsed / SAMPLES << std::endl;
}
```

ps：请忽略源问题代码转置的逻辑错误。

## 简单分析

仅从算法的角度分析，显然循环体内部执行了三条语句，设为`(load, loadstore, store)`三条语句，而一共执行了` N * N `次循环体内部的语句，显然时间为`N * N * (time(load), time(loadstore), time(store))`。因此 512 * 512 矩阵转置比 513 * 513 转置更快的。

## 运行结果

在我的机器上：

- macOs Catalina
- 1.4GHz 四核 Intel Core i5
- Apple clang version 11.0.0
-  x86_64-apple-darwin19.6.0

但是实际执行结果是这样的：

```shell script
Average for a matrix of 512: 1043
Average for a matrix of 513: 837
```

 这是开启`-O2`优化的情况：

```shell
Average for a matrix of 512: 1621
Average for a matrix of 513: 419
```

性能差异更大。也就是说，运行结果与理论分析出现了偏差，回顾一下时间分析公式——`N * N * (time(load), time(loadstore), time(store))`，显然` N * N `这个部分不是性能差异的关键，否则 512 * 512 矩阵转置会比 513 * 513 更快，所以问题出现在  `(load, loadstore, store)` 指令的执行时间上。

## 缓存

`(load, loadstore, store)`这三条指令都是访存指令，我们可以根据[数据原先地址，数据存放位置]分为几类：

1. 磁盘--内存
2. 内存--内存
3. 内存--缓存

前者是从磁盘中读取数据到内存，而此代码不涉及到IO；后者是访存导致缺页加载物理页的操作，然而在计时之前（L26之前）初始化的过程已经完成了物理页加载的操作；最后是内存到缓存的转移。

在现代CPU中，一般会有多级cache，一般分为L1，L2，L3 cache，不同级别的cache有着不同的访问延迟，如下图：

![Applied C++: Memory Latency. Benchmarking Kaby Lake and Haswell… | by  Andriy Berestovskyy | Applied | Medium](https://miro.medium.com/max/1840/1*ns6F6cMmInDcCZ_Tsha0yA.png)

图片[ref](https://medium.com/applied/applied-c-memory-latency-d05a42fe354e)。

可以看到，离CPU core约近，需要的访问时间也就越短，因此同样是`load`操作，其延迟也会有很大的差距。我们以我的机器举例子，

```shell
$ sysctl hw.l1dcachesize    # l1 data cache
hw.l1icachesize: 32768
```

![image-20201114142408960](%E7%9F%A9%E9%98%B5%E8%BD%AC%E7%BD%AE/image-20201114142408960.png)

根据[Intel® 64 and IA-32 Architectures Optimization Reference Manual](https://www.intel.com/content/dam/www/public/us/en/documents/manuals/64-ia-32-architectures-optimization-manual.pdf)，可以知道我的L1cache为32kb大小，并且是8路相联，在继续分析矩阵转置的访存时间之前需要介绍一下CPU cache的缓存原理。

### cache原理

> In [computing](https://en.wikipedia.org/wiki/Computing), a **cache** is a hardware or software component that stores data so that future requests for that data can be served faster; the data stored in a cache might be the result of an earlier computation or a copy of data stored elsewhere. 
>
> ​																																-- wikipedia

在计算机中，cache基于计算机中的局部性原理，提前缓存数据，在将来需要用来数据的时候直接提供数据，以提供更快的读数据的速度。从这个角度来看，cache至少需要提供两个接口——`put, get`，而根据cache场景的不同，`put, get`的输入输出以及其实现原理也会不同。

对于CPU L1 cache来说，其`put, get`接口为：

- `put(addr, data)`
- `get(addr) -> data`

其中`addr`为数据所在的地址，`data`为数据。而这还不够，我们还要明确，`addr`是多少位的，`data`是多少位的。

我们知道，对于地址来说，可寻址范围取决于处理器的地址总线的长度，那么一个64位的CPU的地址就应该是64位的，然后`data`多少位我们也可以简单的理解为每次从`cache`里面读64位的地址，但是实际上`data`的长度取决于实现。

也就是说CPU cache需要实现从`addr`到`data`的映射，如果用Hash + 拉链的方式去实现CPU cache，可以这么实现。

- 一个长度为 $ 2^b $ 的数组，存储链表头
- 每个链表节点存储`<addr, data>`

![image-20201114162537846](%E7%9F%A9%E9%98%B5%E8%BD%AC%E7%BD%AE/image-20201114162537846.png)

由于地址本身就是整形数，那么我们不用Hash函数，直接取`addr`中的某`b`位去索引数组下标，然后遍历链表节点，如果`addr`相同，那么就返回其中存储的`data`。

以上是一个合理的使用Hash表的方式实现CPU cache，但是要注意的是在硬件中难以实现链表结构，而且由于用作 cache的原价成本昂贵，不可能无限制的扩展cache，在添加了这两个条件之后，我可以重新设计一下CPU cache结构：

- 一个长度为 $ 2^b $ 的数组，数组元素里面存固定大小的空位
- 空位里面存储`<addr, data>`

![image-20201114163644008](%E7%9F%A9%E9%98%B5%E8%BD%AC%E7%BD%AE/image-20201114163644008.png)

如上图，其实其中`b`为3，在插入`addr1`的时候根据地址的其中3位映射到了`index = 2 `的桶中，找到第一个空桶，然后将数组置入cache中。这样由于cache容量固定，当要存的data数目过多，淘汰哪个元素取决于淘汰算法，比如LRU，LFU等。

这样的实现已经很接近于实际CPU cache的实现了，一个比较典型的CPU cache实现如下：

![img](https://upload.wikimedia.org/wikipedia/commons/thumb/9/93/Cache%2Cassociative-fill-both.png/450px-Cache%2Cassociative-fill-both.png)

图片[ref](https://en.wikipedia.org/wiki/CPU_cache)

左边是主存的结构，右边是cache的结构，CPU cache根据一个bucket里面放多少个元素和一共多少个组分为三种cache：

- **直接映射缓存**，每个bucket里面只能放一个元素，主存直接映射到cache对应的位置中
- **全相联映射缓存**，只有一个bucket，主存的数据可以放到bucket的任何位置
- **N-way组相联映射缓存**，有多个bucket，每个bucket被称为一个组，每个组可以放N个元素

**直接相联**和**全相联映射**可以说是**N-way组相联映射**的两种特殊情况，其中**直接相联**是**1-way组相联映射**，而**全相联映射**是**N-way1组相联映射**，只有一个组。

