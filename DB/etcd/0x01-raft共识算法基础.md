

# Raft共识算法

## 目录

- 共识场景
- Raft算法
  - 概述
  - 选举
  - 日志
  - 安全性
  - 配置更改

## 共识场景

### 单机条件下的一致性问题[^1]

![image-20200416204849622](image-20200416204849622.png)

如上图，一个客户端向一个节点发送更新读取请求，如果服务器是**单线程**处理请求，那么单机环境下的强一致性很好解决，每个用户总是能读取到*请求到达时的最新的数据​*。

### 多机下的一致性问题

而在单机服务的致命弱点在于无法提供**可用性**，如果节点挂掉，那么服务就不可用，所以一般情况下一个服务（比如数据库）都会加上一个从服务器，在主服务器挂掉的时候提升为主节点继续提供服务。

![image-20200416214114619](image-20200416214114619.png)上图是一个典型的主-从服务架构，客户端向主节点发出请求，主节点同步的复制写请求到从节点上，主节点**收到从节点写入成功的响应后**再响应客户端写入成功。一切看似很美好，然而主从节点网络发生中断的时候就必须面临**可用性**和**一致性**的抉择了。

![image-20200416214854258](image-20200416214854258.png)

如上图，如果主节点和从节点的网络断开，此时客户端向主节点发出写请求，那么根据主节点的行为可以分为两种情况：

1. *主节点返回写入成功*。这样提供了**可用性**，但由于网络断开，所以从节点是没有收到同步请求的，那么客户端在收到写成功的响应后再执行读取的话，从节点返回的就不是最新的值，这样就失去了**一致性**。
2. *主节点停止响应*。在网络恢复之前主节点决定不响应客户写请求，这样客户读的总是最新的数据，保证了数据的**一致性**（所有的写操作失败，自然会读到“最新”的数据），然而却失去了**可用性**。

以上的情况也正是是[CAP理论](https://en.wikipedia.org/wiki/CAP_theorem)的内容[^2]——在网络分区的情况下在**一致性和可用性不可兼得**。

### 加上容错的强一致性模型

可以看到，简单的主从模型并不能在拥有**一致性**的情况下保证**可用性**。根据失效节点类型可以分为两种情况：

1. *从节点失效*。这种情况下主节点可以继续执行写操作，同时兼顾读操作，只要主节点不失效，那么从节点无论失效多少个，集群依然能够提供**一致性和可用性**。
2. *主节点失效*。这种情况下，普通的主从复制模型并没有很好的办法，如何判断主节点失效、如何选取主节点、如何确保选取的新主节点能让系统保证**一致性**，这些问题在普通的主从复制中没有很好的答案。

所以，普通的主从复制模型只能在没有网络分区的情况保证一致性的情况下提供**从节点失效的容错性**，而主节点失效会直接导致系统瘫痪。也就是说，如果能够解决主节点失效的问题，我们就可以拥有**带容错的一致性系统**。

选取主节点是一个**共识问题**——*所有节点就“谁是主节点”达成共识*。选主这个问题可以分解为以下的几个问题：

- 如何判断主节点失效
- 如何选取主节点（主节点需要满足什么条件）
- 为了失效自动选举主节点，在非失效的情况下的复制操作需要加入什么样的变化（[这篇总结]([https://qinggniq.com/2019/11/11/DDIA%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E7%AC%AC%E5%85%AB%E7%AB%A0-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E7%9A%84%E6%8C%91%E6%88%98/](https://qinggniq.com/2019/11/11/DDIA读书笔记——第八章-分布式系统的挑战/))提到的在分布式场景中可能出现的两种问题**不可靠的时钟**和**不可靠的网络**，消息可能错发，可能重发，如何避免这些问题）

解决这类**共识问题**的算法叫做**共识算法**，[Raft](https://en.wikipedia.org/wiki/Raft_(computer_science))便是其中一种。

## Raft算法[^3]

Raft算法被提出的动机是提出一种**易于理解**的**共识算法**。**易于理解**是相对于[**Paxos算法**](https://en.wikipedia.org/wiki/Paxos_(computer_science))来说的。

### 概述

Raft通过选出一个**Leader**，客户端的所有命令都需要直接或间接地发送到**Leader**，**Leader**负责管理命令的顺序，并且将这些命令写到日志里面，并且发送给其他节点使集群节点中的日志达成一致。

开始的时候集群中不存在**Leader**，集群需要自行选举出**Leader**，选举出**Leader**之后，**Leader**需要通过心跳来保持**Leader**的权威性，其他节点也是通过心跳来确定当前集群中的**Leader**是否存活，如果一定时间收不到**Leader**的心跳，那么这个节点就需要开始选举过程来确定新的**Leader**；除此之外，**Leader**同时需要负责接受客户端的情请求，确定请求的顺序，并且同步给其他节点，通过节点的回复确定哪些命令可以被**提交**[^todo]。

所以，Raft的核心部分包括三个个部分：

1. *选举*：选取集群中的主节点的步骤
2. *日志复制*：客户端发送命令给主节点、主节点发送日志给从节点期间的步骤
3. *安全性*：Raft算法在执行的过程中有很多特性必须一直保证，安全性就是证明在不同的情况下Raft算法可以一直保证这些特性，这些特性也是Raft算法能够在不可靠的分布式系统环境中实现正确的共识的原因

### Raft基础

直观的看**选举**和**日志复制**两个过程需要四种消息格式：

- *请求投票*：节点想要竞选主节点，就要发送其他节点“请求投票”消息来获得其他节点的支持
- *投票响应*：这个节点想要获得我的选票，我同意还是拒绝，作为“请求投票”消息的响应
- *日志复制*：成为主节点后，客户端发给主节点命令，主节点需要发送“日志复制”消息来同步日志
- *复制响应*：作为*日志复制*消息的响应

### 选举

Raft将节点分成了三种不同的角色，分别是**Leader**、**Candidate**和**Follower**，通常情况下一个集群里面只有一个**Leader**和其他的**Follower**，只有在**选举**的时候才会出现**Candidate**。

![image-20200420163847614](image-20200420163847614.png)

Raft把时间通过一次次**选举**划分为不同的`Term`（任期），每次选举都会使`Term`增加，如果选举出`Term`，那么**Leader**将会在这个`Term`期间管理整个集群，而如果**选举**因为没有选出**Leader**那么这个`Term`也会因为没有**Leader**而结束。

![image-20200420202759746](image-20200420202759746.png)下面我们以三个不同状态的节点的伪代码来解释状态转移过程（在下面的伪代码中，会忽略除了选举过程的其他处理）：

#### Follower处理流程

```go
//state Follower
开启一个随机定时器
for {
  if 超时 {
    Term++
    成为Candidate
  }
  if 收到其他节点的消息 {
    if 是Candidate的“请求投票”的消息 或者 是Leader的“日志复制”{
      重置定时器
      作出回应
    }
  }
}
```



#### Candidate处理流程

```go
// state Candidate
开启随机的选举定时器
for {
  发给所有节点“请求投票”消息
  if 超时 {
    重置定时器
    重置投票信息
    开启新一轮的选举
  }
  if 收到其他节点的消息 {
    if 是“请求投票”的响应 并且 获得了大多数节点的同意 {
    	成为Leader
  	}
    if 是新Leader的心跳 或者 消息的Term更大 {
      成为Follower
    }
  }
}
```

#### Leader处理流程

```go
//state Leader
开启心跳定时器
发送心跳给其他节点
for {
  if 超时 {
    发送心跳给其他节点
  }
  if 收到消息 {
    if 消息的Term更大 {
      成为Follower
    }
  }
}
```

可以看到`Term`使**Leader**转化到**Follower**、**Candidate**转化到**Follower**的关键。





#### 通过复制实现可用性

在分布式系统中，多个存储独立的机器想要在数据上达到一致性，就必须通过复制的方式。[**复制状态机[State machine replication]**](https://www.cs.cornell.edu/fbs/publications/ibmFault.sm.pdf)是实现分布式容错系统的一个基本的方法[^4]。

## ps

[^1]: 在这里一致性只是简单的指用户视角下的强一致性，也就是用户的读取看到的数据总是最新的.

[^2]:Seth Gilbert and Nancy Lynch. 2002. Brewer’s conjecture and the feasibility of consistent, available, partition-tolerant web services. SIGACT News 33, 2 (June 2002), 51–59. DOI:https://doi.org/10.1145/564585.564601

[^3]:Diego Ongaro and John Ousterhout. 2014. In search of an understandable consensus algorithm. In Proceedings of the 2014 USENIX conference on USENIX Annual Technical Conference (USENIX ATC’14). USENIX Association, USA, 305–320.

[^4]: https://en.wikipedia.org/wiki/State_machine_replication



[^todo]: 