# DDIA 分布式数据系统-数据复制
**多台机器上数据存储的挑战**
* 扩展性：读写数量负载过大，超出当前机器的上限，需要水平扩展
* 容错和高可用：单台机器出现故障，系统可以继续工作
* 延迟考虑：CDN，选择附近的节点提供服务

**垂直扩展的局限性**
- 共享**内存**式的垂直扩展：价格和性能不是正比关系
- 共享**磁盘**式的垂直扩展：由于资源竞争和锁开销导致实际上不太行

**无共享结构（水平扩展）**
- 以**网络**进行通信：性价比高，容错，延迟低。复杂性

**复制和分区**
- *复制*：在多个节点保存相同数据的副本，提供冗余。提供容错和系统性能。
- *分区*：将大数据分成小的存在不同的节点上。
## 0x05 数据复制
**数据复制的目的**
- 物理位置更接近用户，以降低延迟
- 容错性
- 提高吞吐量

**三种复制数据变化的方法**
- 主从复制
- 多主节点复制
- 无主节点复制

### 主节点和从节点（主从复制）
**复制原理**
1. 指定一个副本为主节点。客户端把写请求发给主节点，主节点先把新数据存到本地。
2. 其他副本为从节点。主节点写完自己的后将数据更改作为**复制日志**或**更改流**发送给所有从节点。从节点收到后应用到本地并**严格和主节点写入顺序一致**。
3. 从节点只读，主节点可读可写。

### 同步复制和异步复制
- **同步**：等到从节点复制成功再向客户端返回。
- **异步**：不等从节点的回应直接返回。

一般开启同步只是说一个从节点是同步，其他的都应该是异步，一旦那个同步的节点失效了，可以将一个异步的节点提示为同步节点。
### 配置新的从节点
需要添加新的从节点时，如何保证主节点和从节点数据一致？

1. 在某个时间点产生一致性快照
2. 拷贝快照到新的从节点
3. 从节点请求快照之后的数据更改日志。PostGRESQL叫"log sequence number"，MySQL叫"binlog coordinates"
4. 获得日志后开始执行日志上的数据更改，也叫**追赶**

### 处理节点失效
**从节点失效：追赶式恢复**
从节点失效后就失效了，恢复直接按照**配置新的从节点**那样恢复就好了。

**主节点失效：节点切换**
步骤：
1. *确认主节点失效*：心跳，超时。
2. *选举新的主节点*：一致性共识算法它lei了，目标就是找到和失效主节点数据相差最小的从节点，把它变成主节点。
3. *重新配置系统使得新主节点生效*：旧主节点再次上线得保证它降级为从节点。

这里面又有很多问题：
- 原主节点上没有复制的数据如果保留的话那么可能会导致写冲突
- 如果丢弃数据并且还和其他依赖数据库的内容协作，那么就十分危险
- 脑裂
- 超时时间设置导致的问题

### 复制日志的实现
主要有四种：
- 复制写操作命令
- WAL，复制磁盘的变化记录（看看WAL的细节）
- 关系型数据库中复制表中行的变化
- 基于触发器的复制

**复制写操作**

就是把`update, insert, delete`的语句发送给其他节点。

问题在于有些写操作不适合复制：
- 调用非确定性函数的语句，比如`Now()`。
- 使用了自增列，必须得保证所有的节点按照完全相同的顺序执行，否则就会产生不一致结果
- 有副作用的语句，触发器、存储过程、用户定义函数

**基于预写日志（WAL）传输**

记录磁盘块哪些字节发生变化，这个和存储引擎紧密耦合，升级的话得停止服务。

**基于行的逻辑日志复制**

记录行的变化，这样逻辑日志和存储引擎解耦，所以比较容易保持向后兼容。

**基于触发器的复制**

只复制想复制的数据，但是开销比较大。

### 复制滞后的问题
复制很慢，客户端从从节点可能看到过期的信息。

**最终一致性定义**
> 这种不一致只是暂时状态，如果停止写数据库，经过一段时间后，从节点最终会赶上主节点并与主节点保持一致。这种效应也叫作最终一致性。

#### 读自己的写
由于**复制滞后**的问题可能导致一个用户在主节点提交了自己的修改操作而在从节点读不到自己的写，那么用户会很生气。

解决方法：
- 如果客户访问可能会被修改的内容，那么从主节点读取；否则从从节点读取。比如用户只会修改自己主页上的东西。
- 大部分的内容用户都可以修改的话就不行了，那么可以根据更新时间，比如1分钟内的读就从主节点读，否则从从节点读。
- 记住数据副本最近更新的时间戳，如果从节点的不够新，就交给其他从节点处理。
- 如果在多数据中心就得把请求路由到主节点所在的数据中心。

还有一些其他问题，比如跨设备共享。

#### 单调读
如果用户读两次，结果之前看到的数据没有了，那么就不是单调读，可能是两次读的请求的不是一个从节点。解决方法可以根据用户IDhash路由一下。

#### 前缀一致读
两次的写由于网络原因导致实际的写入顺序不一致，比如B是A的回答，结果回答先写入，问题后写入，这样就会导致因果倒置。这种情况在**分区**数据库中出现，因为只是复制的话，那么一致性保证它们的写入顺序是和主节点是一个顺序的。

### 复制滞后的解决方案
取决于你的业务要求，比如**写后读**提供了比**最终一致性**更强的一致性要求，但是必然会带来性能的损失和结构的复杂化，那么是否选择取决于你的业务。

### 多主节点复制
由于单主节点复制架构下所有的写操作都是通过的主节点，如果主节点失效，那么会影响所有的写入操作，多主节点可以解决这个问题。
#### 适用场景
普通的一个数据中心不适合多主复制，因为引入的复杂度大于它所带来的好处了。一下几个场景比较合适。

**多数据中心**

一个数据中心相当于一个局域网的集群，那么多数据中心相当于在多个局域网里面分别配置一套主从复制结构，然后每个数据中心的主节点接受写请求，然后异步复制到其他数据中心的主节点。相比于普通的主从复制
- *性能*：普通的主从复制会导致写请求发向里的比较远的主节点，而多主节点能充分利用地理位置的优势，用户体验更好。
- *容忍数据中心失效*：主从复制主节点失效必须迁移到另一个从节点，而多主节点可以使每个数据中心独立运行。
- *容忍网络问题*：由于广域网不如局域网稳定，所以普通主从复制由于是同步复制，所以更加依赖于数据中心，而多主节点因为是异步复制，所以网络中断不会影响写入成功。

但是多个数据中心可能会产生写一份数据的问题，必须处理写冲突。

**离线客户端操作**

有些应用允许用户离线做的更改在上线的时候立刻同步到服务器和其他设备，所以每个设备相当于一个主节点，所有设备之间使用异步复制的方式同步这个多主节点的副本。但是网络连接十分不可靠。

**协同编辑**

需要处理写冲突。
#### 处理写冲突
两个不同客户在不同的数据中心往一个记录写了不同的数据，由于不同数据中心是异步复制，所以在开始是不会察觉出来的。然而在写的时候不同的顺序会导致不同的结果。
**避免冲突**
设置对于特定写请求只通过一个主节点就可以避免。有时候主节点失效了需要重新路由。

**收敛于一致性状态**
解决方式
- 给每个写入分配一个唯一的ID，挑最高的胜出，如果基于时间戳，那么容易造成数据丢失。
- 为每个副本分配一个唯一的ID，制定规则，也容易造成数据丢失。
- 以某种方式将这些值合并，比如B、C存为“B/C”
- 预定格式保存冲突信息，依靠应用层逻辑解决。

**自定义冲突逻辑**
让用户编写代码解决冲突
- *在写入的时候执行*：只要数据库检测到写入冲突，那么就调用用户写的冲突处理脚本。
- *在读取的时候执行*：在写入的时候把所有的数据保留，下次读取时把所有的数据版本返回给用户。应用层自己解决冲突。

#### 拓扑结构
复制自然有复制的方法，比如环状，就是从一个主节点到另一个主节点到最后一个主节点一直复制；比如星形，就是从一个主节发给其他所有主节点；比如全连接，从一个主节点到其他主节点，然后其他主节点再发给所有其他主节点。

环形和星形的不足就是可能会无线循环，所以需要标识符来表示是否通过，并且如果一个节点失效了，那么它们的转发可能也会失效。

但是全链接的结构可能发生日志覆盖的情况，由于网络速度不同导致的顺序不一致。

比如一个客户端X insert一个值，然后主节点在同步给其他主节点的过程中先到了B主节点，客户端Y读到了那个值，并且更新了它，发给了B主节点，B主节点又要同步这个更新操作到其他节点，而主节点C的接受顺序可能是更新操作先到而插入顺序后到。

**版本向量**是解决这个的一个方式。

### 无主节点复制
无主没有主节点和从节点的区别，任何一个节点都可以处理写操作。具体的实现方式有：
- 客户端直接发送许多写操作到所有的节点
- 用一个协调者来协调数据的写入，但是不负责维护写入顺序

#### 节点失效时写入数据库
1. 客户端写的时候失效

客户端根本不管那些失效节点无法写入的情况。

2. 节点恢复时，客户端读

此时客户端可能读到不同的值，那么客户端根据版本号来确定哪个是最新值。

**读修复和反熵**
> 就是失效节点恢复后如何赶上其他节点？

- *读修复*：客户端读取后发现某个节点上的数据是过期的，那么把新的值写入数据库。
- *反熵*：后台进程找差异，不能保证以特定顺序写入，会引起明显的同步滞后。

**读写 quorum**
写入需要w个节点确认，读入需要读r个节点，那么r + w > n才能保证读出来的值至少有一个是新值。如果可用节点小于w或r那么可能会发生写入或读取错误。

**Quorum一致性的局限性**
大w和r能保证读取和写入的正确性，小的w和r可以获得更快的读取写入速度。然而即使是$w + r > n$的情况下依旧可能发生返回旧值的边界条件：
- 使用了*sloppy quorum*，写的w节点和读的r个节点可能完全不同，无法保证有重叠的节点。
- 如果两个写操作同时发生，无法明确先后顺序。唯一的方法就是合并写入，如果根据时间戳决定，那么由于时钟偏差的问题一些写入可能被错误的丢弃。
- 太多不写了

**sloppy quorum**
- 如果无法到达w或r所要求的quorum，如何明确把错误返还给客户端？
- 或者我们接受读请求，不过只是把它们放到一些可以访问的节点中？这些节点可能不在规定的n个节点中。

*sloppy quorum*对提高写可读性很有用，但是不能保证读到最新值，因为之前规定
的w和r不包括那些临时存储的节点。

**多数据中心操作**
有的模型跨数据中心，有的只能在单个数据中心设置quorum值。

#### 检测并发写
写冲突在无主结构里面依然会出现，比如某个图。
![并发写入](https://github.com/qinggniq/Note/blob/master/GO/MIT6.824/imags/%E5%B9%B6%E5%8F%91%E5%86%99.png)
最后节点1\2\3对于X的值的结果不一致。之前写冲突有一些简单的解决方法，ID时间戳获胜、全部保留、副本ID、应用层解决。

**最后写入优先**
如何定义最新。由于支持并发写，那么哪个先发生就没什么意义了，所以可以对其强制排序，最后的一个获胜。牺牲了持久性，即使那些写入都对客户端报告成功，但是最终也只有一个值保留下来。

**happens-before关系和并发**
如何判断两个操作可不可以并发？
- 某个操作建立在另一个操作上不能并发。
- 操作之间不存在因果关系可以并发。

> 讨论：并发的定义是啥？

和时间无关，两个操作不需要意识到对方，那么我们可以称它们是并发操作。

**确定前后关系**
操作的并发处理主要靠版本号解决。

