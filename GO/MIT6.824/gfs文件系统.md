# Google File System
## 前提
1. 组件失效被认为是常态事件,而不是意外事件。
2. 以通常的标准衡量,我们的文件非常巨大。数 GB 的文件非常普遍。
3. 绝大部分文件的修改是采用在文件尾部追加数据,而不是覆盖原有数据的方式。
4. 应用程序和文件系统 API 的协同设计提高了整个系统的灵活性。
## 设计概述
### 设计预期
1. 系统由许多廉价的普通组件组成,组件失效是一种常态。系统必须持续监控自身的状态,它必须将组件失效作为一种常态,能够迅速地侦测、冗余并恢复失效的组件。
2. 系统存储一定数量的大文件。我们预期会有几百万文件,文件的大小通常在 100MB 或者以上。数个 GB大小的文件也是普遍存在,并且要能够被有效的管理。系统也必须支持小文件,但是不需要针对小文件做专门的优化。
3. 系统的工作负载主要由两种读操作组成:大规模的流式读取和小规模的随机读取。小范围的读取参考**莫队算法**。
4. 统必须高效的、行为定义明确的 2 实现多客户端并行追加数据到同一个文件里的语意。

===== 看莫队

### 结构
- 一个master多个chunkserver，如果master里面存chunk的元信息

client => master
(filename, chunk index)
master => client
(chunk handler, chunklocations)

master => chunkServer
instruction to chunkServer

chunkServer => master
chunServer State

- 元信息存在Master的内存中
- client会根据(filename, chunk index)缓存信息，因为client可能一次发出多个请求

### 元数据
**master**
- 文件
- Chunk的命名空间
- 文件和Chunk的对应关系
前两种类型的元数据 8 同时也
会以记录变更日志的方式记录在操作系统的系统日志文件中,日志文件存储在本地磁盘上,同时日志会被复制到其它的远程 Master 服务器上。

### Chunk 位置信息
Master 服务器并不保存持久化保存哪个 Chunk 服务器存有指定 Chunk 的副本的信息。Master 服务器只是在启动的时候轮询 Chunk 服务器以获取这些信息。心跳监控ChunkServer的状态。

> 最初设计时,我们试图把 Chunk 的位置信息持久的保存在 Master 服务器上,但是后来我们发现在启动的时候轮询 Chunk 服务器,之后定期轮询更新的方式更简单。这种设计简化了在有 Chunk 服务器加入集群、离开集群、更名、失效、以及重启的时候,Master 服务器和 Chunk 服务器数据同步的问题。

> 只有 Chunk 服务器才能最终确定一个 Chunk 是否在它的硬盘上。我们从没有考虑过在 Master 服务器上维护一个这些信息的全局视图,因为 Chunk 服务器的错误可能会导致 Chunk 自动消失(比如,硬盘损坏了或者无法访问了),亦或者操作人员可能会重命名一个 Chunk 服务器。
### 操作日志
操作日志包含了关键的元数据变更历史记录。这对 GFS 非常重要。这不仅仅是因为操作日志是元数据唯一的持久化存储记录,它也作为判断同步操作顺序的逻辑时间基线  。

Master 服务器在灾难恢复时,通过重演操作日志把文件系统恢复到最近的状态。为了缩短 Master 启动的时间,我们必须使日志足够小  。Master 服务器在日志增长到一定量时对系统状态做一次 Checkpoint 11 ,将所有的状态数据写入一个 Checkpoint 文件 12 。在灾难恢复的时候,Master 服务器就通过从磁盘上读取这个
Checkpoint 文件,以及重演 Checkpoint 之后的有限个日志文件就能够恢复系统。Checkpoint 文件以压缩 B-树形势的数据结构存储,可以直接映射到内存,在用于命名空间查询时无需额外的解析。

**新开线程进行checkpoint的操作**

### 一致性模型
记录追加方式的“至少一次追加”的特性保证了 Writer 的输出。Readers 使用
下面的方法来处理偶然性的填充数据和重复内容。Writers 在每条写入的记录中都包含了额外的信息,例如Checksum,用来验证它的有效性。Reader 可以利用 Checksum 识别和抛弃额外的填充数据和记录片段。如果应用不能容忍偶尔的重复内容(比如,如果这些重复数据触发了非幂等操作),可以用记录的唯一标识符来过滤它们
## 系统交互
我们在设计这个系统时,一个重要的原则是最小化所有操作和 Master 节点的交互。
## 租约(lease) 19 和变更顺序



=========================分界线==================
## 问题
### 租约
> 即使Master 节点和主 Chunk 失去联系,它仍然可以安全地在旧的租约到期后和另外一个 Chunk 副本签订新的租约


> 租约是个啥
> 客户端写请求 和 租约的对应关系是什么， 1对N、N对1、N对M？

实际数据 和 写入顺序 是不一样的东西。

1. client请求chunk位置，Master返回主Chunk（拥有租约的chunkServer）
2. client将数据发送到那些chunkServer，所有Server接受到数据后，client发出写操作，主ChunkServer就会把收到的所有写操作存储分配序列号。
3. 发送这个操作序列给其他server。

**好处**
将控制延迟给chunkServer去做，减小Master的网络负载。
如果失败，客户端会重试

## 数据流
**注意一点**，client的数据不是直接发送到所有的备份上的，而是先发到主ChunkServer上，然后沿着一条线路一直发。通过IP计算距离。

**好处**
树状的发送方式利用入口带宽，链状方式利用出口带宽。

## 快照
- 快照请求
- 取消租约
- 开始拷贝
- 当新的写请求到来，发现这个Chunk的引用计数大于1，那么拷贝一下，并且向所有拥有备份的发出拷贝请求，返回新的Chunk的hander。

# Master节点操作
有些操作比如快照，耗时过长，那么不希望这个操作影响其他操作，那么就给哪个目录加锁，由于gfs用的树形结构来加锁，所以在备份`/home/qinggniq/foo`这个文件的时候，需要获得`./home, //home/qinggniq/,`的读锁，和`/home/qinggniq/foo`的写锁。
## 创建、复制、负载
主要为了最大化利用率，选择合适的位置存储不同的副本；
在副本数过少的时候选择复制副本来增加容错性；
通过调整位置来提高速度；
## 垃圾回收
惰性删除。
**好处**
1. 组件失效是常态，组件失效的时候可能丢失删除信息
2. 分散开销，在master空闲的时候再去删除
3. 避免意外的删除操作

## 过期失效的副本检测
每建立一个租约，增加版本号，持久化版本号，根据版本号来判断是否失效。

# 高可用性
## 快速恢复

## chunk复制
因为有三个副本有数据，所以在一个副本失效后（过期、离线、数据错误），完全复制其他副本。
> checkpoint文件里面到底存储了什么？


## master节点的恢复
处于 GFS 系统外部的监控进程会在其它的存有完整操作日志的机器上启动一个新的 Master 进程


影子master，只读

## 数据完整性
checksum，只在单机上比较

不会太影响性能，因为计算和IO可以同时进行。

