# 数据分区
数据复制基本只能解决容错备份问题，然而对于海量数据集或非常高的查询压力，复制技术不能解决问题，我们还需要将数据分区。
> 分区通常是这样定义的，即一条数据（或者每条记录，每行或每个文档）直属于某个特定分区。

数据分区将文档的不同部分放到不同的机器上，这样请求负载也可以分配到更多的处理器上，在单个分区进行查询时，每个节点对自己的分区可以进行独立的查询操作，所以添加更多的节点可以提高吞吐量。

## 数据分区和数据复制
它们俩比较独立，一般数据分区伴随着数据复制来提高系统容错性（参考Google File System和其工程实现HDFS）。所以一个节点可以存多个分区，一个分区可能被多个节点存储。

## **键-值数据的分区**
（？？？）难道是关系型数据库不适合分区，还是书上讲的分区方法不适合以表为单位的分区操作？
分区的目的是**将数据和查询负载尽量均匀分布在所有节点上**，尽量均匀的意思是不要产生热点的情况，所以最简单的方法是**随机分配**，缺点是没有记录和节点间的对应关系，不得不并行查询所有节点。
### **基于关键字区间分区**
为每个分区分配一段连续的关键字或者关键字区间范围，类似于新华字典，A字母开头的单词一个分区，B字母开头的另一个分区。
优点：
- 可以在分区内根据关键字排序，然后区间查询十分方便
缺点：
- 某些访问模式可能会造成热点，比如测量数据如果以时间为关键字，那么今天的数据就成了热点。

解决方法是用其他的内容作为关键字比如传感器名字，然后再用时间戳在分区内部排序，这样查询今天的测量数据的时候就可以在各个分区查询今天的数据了。

HBase，RethinkDB，2.4之前版本的MonogoDB。
### **基于关键字哈希值分区**
为了避免数据倾斜与热点问题，可以使用基于关键字哈希函数的方式来分区。

找到一个合适的哈希函数，为每个分区设置一个哈希范围，关键字根据哈希值分配到不同的分区中。虽然均匀效果很好，但是失去了良好的区间查询的能力。（一致性哈希可以解决节点失效导致的数据倾斜的问题）

Cassandra使用在两个分区策略中采取了折中。它可以讲表声明为由多个列组成的复合主键。复合主键只有 第一部分用于哈希分区，其他列可以用来对Cassandra的SSTable中的数据排序。类似于传感器的例子。

### **数据倾斜与热点**
但是无论是基于关键字区间分区还是基于哈希函数的分区，还是解决不了一些热点的情况，比如一个微博名人发布一些热点事件就可能引发一场访问风暴，出现大量关于此关键字的写操作。哈希解决不了任何问题。

一个简单的解决方法是在关键字的开头或结尾加一个随机数，但是这样以后的每个读操作都得涉及到多个节点的读取合并操作，所以这种方法只适合用在热点的关键词上。需要开发者自己来权衡设计。

## **分区和二级索引**
关系型数据库中二级索引是标配，文档数据库也比较普遍，但是由于它的复杂性，大部分键-值数据库不支持二级索引。二级索引带来的问题是**它们不能规整地映射到分区中**。

### **基于文档分区的二级索引**
![基于文档分区的二级索引](https://github.com/qinggniq/Note/blob/master/GO/MIT6.824/imags/基于文档分区的二级索引.png)
当一个新的数据加入的时候，数据库会将数据加入到对应的文档ID列表。（？？？）

写入、更新、删除的时候只需要处理包含目标文档ID的那个分区，因此文档分区索引也叫作本地索引。
但是查询的时候比如查询红色的汽车则需要查询所有的分区，然后合并结果。

MonogoDB、Riak、Cassandra、Elasticsearch、SolrCloud、VoltDB都支持基于文档的二级索引，

### **基于词条的二级索引分区**
对所有的数据构建索引，而不是每个分区各个维护，而且全局索引不能维护在一个节点上面，否则破坏了分区平衡的目标。(https://github.com/qinggniq/Note/blob/master/GO/MIT6.824/imags/基于词条的二级索引分区.png)

索引的划分策略可以通过之前提到的基于关键词区间和基于哈希的方法，它们的优缺点也和之前提到的一致。

基于词条的方法查询比较方便，直接根据二级索引关键词向索引所在的节点查询数据所在的分区。但是写入操作十分复杂，一条记录的写入可能涉及到多个二级索引，那么需要全部更新，而且那些二级索引一般不在一个节点上，会引起写放大。所以一般异步更新。


## **分区再平衡**
由于节点的数量和数据不会保持恒定，比如一个节点上的数据越来越多，之前设计的划分策略不能达到平衡的目标了，需要再平衡。
- 查询压力增加，需要更多CPU。
- 数据规模增加，需要更多磁盘和内存。
- 节点故障，需要其他机器接管失效节点。
这些变化都要求数据和请求可以从一个节点转移到另一个节点，这个过程被称为**再平衡**，平衡要基本满足以下的性质：
- 平衡后，负载、数据存储、读写请求应该在集群内更均匀的分布；
- 不能中断服务；
- 避免不必要的负载迁移，以加快再平衡，并尽量减少网络和磁盘IO的影响。


### **动态再平衡的策略**
**为什么不取模**

取模的话如果节点数量发生了变化，发生的数据迁移过大 。

**固定数量的分区**
**先创建远超实际数量的分区，然后给每个节点分配多个分区。**这样再平衡的时候选中的分区会发生迁移，但是不会改变分区和关键字的关系，只需要改变节点和分区的关系，可以逐步完成数据迁移（类似于渐进式rehash），这样旧的分区依然可以接受请求。

选择合适的分区数量是个难题，太大的话每次再平衡和数据迁移量代价就很大；如果过小，就会产生太多开销。（？？？什么开销）

**动态分区**
对于采用关键字区间分区的数据库，如果边界设置有问题，就可能会出现数据挤在一个分区里面的情况，然而设置固定边界、固定数量的分区十分不便，手动重新配置又十分繁琐。

HBase、RethinkDB使用的动态创建分区，当**分区的数据操作一个可配置的阈值，就拆分为两个分区，每个一半，相反如果缩小到一定值，那么就和其他的分区合并**（有点类似于物理内存管理和B树）。这样比较方便。

但是对于一个开始为空的数据库，不知道怎么设定边界，所以会从一个分区开始，在分裂之前的所有读写请求都由一个节点处理，其他的处于空闲状态。HBase和MonogoDB允许在一个空数据库上设置一些空的初始分区。

**按节点比例分区**（？？？没动啥意思）
使分区数和集群节点成正比关系，每个节点具有固定数量的分区。所以分区的大小和数据量的大小成正比，而节点数目增加的时候需要分区大小就会变小。当新节点加入的时候随机选择固定数量的现有分区进行分裂，然后拿走这些分区的一般数据。随机选择分区边界的前提要求采用基于哈希分区，这种方法最符合一致性哈希。

### 自动还是手动再平衡操作
没啥可说

## **请求路由**
知道数据分区到多个节点了，如何知道请求归属于哪个节点，如果发生了分区再平衡，那么它们的关系还会发生变化。

这是一类典型的**服务发现**的问题，服务发现不仅仅局限于数据库，任何通过网络访问的系统都有这样的需求，特别是当服务目标支持高可用的时候。有以下几种不同的策略：
1. 客户端发送请求到任意节点，然后节点自己转发到相应节点；
2. 所有的客户端发送到一个路由，然后路由决定转发给哪个间节点；
3. 客户端感知分区和节点的关系，然后直接发送请求到相应的节点。

[请求路由方式](https://github.com/qinggniq/Note/blob/master/GO/MIT6.824/imags/请求路由方式.png)


Zookeeper有解决的方法，每个节点向Zookeeper注册自己，参与者可以订阅这些位置关系，然后分区变化后，Zookeepr会主动通知到路由层。

Cassandra和Riak使用gossip协议来同步集群状态变化。虽然这样增加了复杂度，但是避免了对于Zookeeper的外部依赖。

