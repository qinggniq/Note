# 分布式系统的挑战

开发者的核心任务是构建可靠的系统，即使系统面临着各种出错的可能，也需要完成预定的工作。

本章对分布式系统可能出现的故障做了一个全面的、近乎悲观的总结，故障可能来自：

- **网络问题**
- **时钟和时序问题**

并讨论这些问题的可控程度，接下来会探讨如何认清分布式系统的状态本质，并以此来评估发生的各种故障。

## **故障和部分失效**

单机上的错误一般是“要么工作，要么出错”这种模式，一般不会出现中间形态。然而多节点的时候会出现系统的一部分正常一部分不正常的现象，我们称之为“部分失效”，并且这种部分失效是不确定的，如果涉及到多个节点和网络，几乎肯定会碰到有时网络正常有时候不正常的情况。

这种不确定性和部分失效极大的提高了分布式系统的复杂性。

## **不可靠的网络**

无共享、使用网络通信是构建集群的主流方式，主要因为：不需要专门的硬件，价格低廉；可以使用跨区域的多数据中心来实现高可用性。

网络一般是异步网络，只发送但是并不保证一定可以到到，有可能：

1. 请求丢失（拔了网线）；
2. 请求在某个队列中；
3. 远程接受节点失效；
4. 远程节点暂时无法响应（垃圾回收，进程暂停）；
5. 响应在网络中丢失（交换机配置错误）；
6. 响应被延迟处理（网络或发送者的机器超出了负载）。

发送者拥有的唯一信息是，尚未收到响应，但无法判断具体原因。所以一般使用超时机制来解决，但是超时也无法确定远程节点是不是收到了请求。

### 现实中的网络故障

现实中网络故障十分普遍。

### **故障检测**

现实中有很多需要检测节点失效的情况：

- 负载均衡器需要避免向失效节点发送消息；
- 主从复制节点的时候需要检测主节点失效然后替换主节点；
- 由于网络的不确定性导致判断网络失效非常困难；
- 如果节点在处理请求的时候发生了崩溃，那么很难知道节点实际处理了多少数据；
- 如果服务器进程崩溃，操作系统依然正常运行，那么可以通过脚本通知其他节点。HBase使用了这种方法；
- 可以通过数据中心的网络交换机的管理接口查询是否存在硬件级别的链路故障；

如果想知道请求是否执行成功，只靠TCP的确认报文是不能确定的，只能靠应用级别的回复。

### **超时与无限期的延迟**

超时过长或过短都会有问题，过长意味着长时间的等待才能确认节点失效，过短可能会造成误判（比如因为网络延迟）。

一个节点失效会将这个节点的负载移到另一个，可能会导致级联级别的失效。

设$d$为网络保证可以交付的最大延迟时间，$r$为一个非故障节点能在期间完成处理的时间，那么$2d + r$是比较合适的超时时间。

**网络拥塞和排队**
可能在**交换机、操作系统、虚拟机切换、流量控制（比如nagel算法）**这些地方等待。TCP的重传也会带来延迟。

> 如果对于一个应用来说重传没有什么意义，那么建议使用UDP

还有可能在共享的网络，其他人的使用会给自己带来巨大的延迟波动。

根据以上的情况，设置超时还是不要设置为定时，而应该随着持续的响应时间的变化动态决定。比如 **Phi Accrual** 来确定超时时间，Akka 和 Cassandra 和 TCP 都使用的是这个。

### **同步和异步网络**

传统的电路交换十分稳定，是因为它为每个信道分配了唯一的通信链路，保证了通信的速度，这种网络本质上是同步的，它不会受到排队的影响，所以延迟是固定的，我们称为**有界延迟**。

但是TCP中的连接不同，TCP 连接的数据包会尝试所有可用的网络带宽，尽力在最短的时间内完成数据发送，当 TCP 连接空闲的时候通常不占用任何带宽。

如果数据中心网络和互联网基于电路交换网络，一旦电路建成可以保证最大往返时间，然而事实上是以太网和 IP 都是基于**分组交换协议**，这种协议注定要受到排队的影响。它不分配电路。

> 为什么数据中心网络和互联网要使用分组交换呢？

它们针对突发流量做了很多优化，电话和视频的网络带宽往往容易事前确定，然而访问网页和传输文件无法事先确定带宽需求，我们只是希望它们尽快完成。

以上便是为什么超时时间设置不能设置为固定值而要动态决定的原因了。

> 分组交换和现代计算机的多核计算机的调度很像，都是充分利用带宽/CPU，达到资源利用率的最大化，这也是**虚拟化**技术的重要动机。

## **不可靠的时钟**

时钟和计时非常重要：

1. 请求是否超时?
2. 服务的99%响应时间是多少？
3. 过去五分钟内，服务平均处理多少请求？
...

在分布式系统中时间是个棘手的问题，跨节点通信不可能即时完成，收到的时间要晚于发送的时间，但是由于网络的不确定延迟性，精确测量面临着很都挑战，使得多个节点通信的时候很难确定事情发生的先后顺序。

每个机器都有自己的时钟，但是设备可能不准确，有的快有的慢，同步时钟的常用方法是网络时间协议（NTP），根据一组专门的时间服务器来调整本地时间，时间服务器则从精确性更高的时间源（如GPS接收器）获取高精度时间。

### **单调时钟和墙上时钟**

现代计算机至少有两个时钟：一个墙上时钟一个单调时钟。

**墙上时钟**
墙上时钟根据某个日历返回当前的日期和时间。Linux 的 `clock_gettime(CLOCK_REALTIME)` 和 Java 的 `System.currentTimeMillis()` 会返回从 1970-01-01 以来的秒数和毫秒数，不含闰秒。

墙上时钟可以和 NTP 同步，但是会有一些问题，比如本地时钟快于 NTP 服务器，那么会强行重置之后回跳到之前的某个时间点，这种跳跃经常忽略润秒，导致不太适合测量时间间隔。

**单调时钟**
Linux 的 `clock_gettime(CLOCK_MONOTONIC)` 和 Java 的 `System.nanoTime()` 返回单调时钟。单调时钟只会向前，不会出现和墙上时钟的回退现象。

如果服务器有多个 CPU 那么每个 CPU 可能有多个单独的计时器，由于线程可能被调度到不同的 CPU 中运行，操作系统这个时候会弥补多个计时器的差，但是最好还是对这个弥补保持谨慎态度。

NTP 同步的时候就不是回退了，而是调整振动频率，然后慢慢和 NTP 服务器同步。

### **时钟同步与准确性**

硬件始终和 NTP 可能会出现一些问题：

- 温度会影响计算机中的石英钟，时钟漂移限制了可以达到的最大精度。
- 如果时钟和 NTP 服务器差距过大，可能会出现拒绝同步或者强制重置。
- 如果和 NTP 链接失败，那么可能会很长一段时间没有意识到错误的配置导致最终同步失败。
- NTP 同步受限于网络环境，由于网络延迟可能产生偏差。
- NTP 服务器配置错误导致偏差。
- 闰秒可能会对没有防备的系统出现混乱。
- 虚拟机的硬件时钟是被虚拟化的，也会出现偏差。
- 如果运行不完全可控的设备上，不能完全相信设备上的时钟。

### **依赖同步的时钟**

和网络问题一样，得假设他们会有问题，但是不同于网络问题，网络问题产生的影响是能够在短期观测出来的，但是时钟的问题不是那么容易观测的，出现的后果可能是隐式的，比如数据库丢失一小部分数据而不是突然的崩溃。比如在多主复制中：

![各个时钟不一致导致的问题](https://raw.githubusercontent.com/qinggniq/Note/master/GO/MIT6.824/imags/DDIA/8-Trasaction/各个时钟不一致导致的问题.png)

写入 `x = 1` 的时间戳为 42.004 秒，写入 `x = 2` 虽然是后续发生，时间戳是 42.003 ，所以节点 2 收到两个事件的时候如果根据 LWW（最后写入胜利）的话，那么会将 `x = 2`的更新丢失。

这是 LWW 会产生的问题，有的实现在客户端生成时间戳，但是这还是无法解决 LWW 会带来的问题：

- 数据库写入可能会奇怪的丢失：后续的写操作不能覆盖之前的写，原因是之前的写时钟过快。这导致了未知的数据被悄悄丢弃，而没有任何报错；
- LWW 无法区分 **快速的连续写操作**和**并发写入**（第五章讲过的并发的定义，就是之前没有因果关系）。需要额外的因果关系跟踪机制（版本向量）来防止因果冲突；
- 两个操作可能产生了一个时间戳，需要额外的仲裁值来确定先后关系，但是这样依然无法区分因果关系。

如果“最新”的定义是**墙上时钟**的话，那么就会引入偏差，即使使用了 NTP 时钟同步，依然可能出现在 100s 的时候发消息，在 99s 的时候收到确认的情况（因为在这段时间里发生了时间同步，导致本地的时钟回退）。

对于排序来讲，基于递增计数器而不是震荡石英晶体的**逻辑时钟**是更可靠的方式，逻辑时钟不是时间，而是时间发生的相对顺序，相对的墙上时钟和递增时钟都属于物理时钟。

**时钟的置信区间**
时间不应该被确定为一个固定值，而应该被视为带有置信区间的时间范围。Google Spanner 的 TrueTime API 会明确告诉本地时钟的置信区间，会得到 [不早于，不晚于] 分别代表误差的最大偏差范围。

**全局快照的同步时钟**
之前提到过的快照隔离级别可以在数据库的某一个状态上不加锁、不违背读写隔离性的前提下高效支持只读事务。

常见的快照隔离实现需要一个单调递增的事务ID，单机上很好实现，只需要一个原子计数器就行了。

但是在分布式的情形下，维护一个全局递增的事务ID就十分困难了，事务ID必须反应因果关系，如果事务 B 要读取事务 A 写的值，那么 B 的事务ID必须比 A 的事务ID大，否则快照不一致。

不能使用墙上时钟作为事务ID，因为时钟不确定性。Google Spanner 使用以下思路来实现跨数据中心的快照隔离。它根据 TrueTime API 返回的时钟置信区间，并基于一下结果来观察：如果 A、B 有两个时钟置信区间，都包含最早和最新可能的时间戳，A = [$A{eraliest}, A{lastest}$], B = [$B{eraliest}, B{lastest}$]，那么如果区间不重叠，即$A{eraliest}< A{lastest}<B{eraliest} < B{lastest}$ 那么可以认定 B 发生在 A 之后。为了确保这个关系，Spanner 在提交读写事务的时候故意等待置信区间的长度。因为Google在每个数据中心部署了一个 GPS 接收器或原子钟，所以 Spanner 的时钟误差范围很小（10ms）。

### **进程暂停**

除了时钟不精确的问题之外，还有一个在分布式系统中危险使用时钟的例子：假设数据库每个分区只有一个主节点，只有主节点接受写入。那么其他节点如何确信该主节点没有被宣告失败，可以安全地写入呢？

一个思路是主节点从其他节点获得一个**租约**（lease），类似于一个带超时的锁（GFS中用到）。某个时间只有一个节点可以拿到，有租约的节点就是这段时间的主节点，为了维护主节点的身份，必须在租约到期之前更新租约，如果节点失效，超时的时候租约失败，那么另一个节点到期之后就可以接管。

```Java
while (true) {
    requst = getIncomingRequest();

    //Ensure that the lease always has at least 10 seconds remaining
    if (lease.expriyTimeMills - System.currentTimeMilliseconds() < 1000) {
        lease = lease.renew();
    }

    if (lease.isValid()) {
        process(request);
    }
}
```

这个代码有什么问题？

- 它依赖于同步的时钟，租约到期时间由其他服务器所设置，如果两个时钟有延迟，那么可能会有问题；
- 如果改为单调时钟，代码假定`if`判断和`process`之间的时间很短，所以设置10s的缓冲区来确保在请求的时候不会过期，但是如果程序执行出现了意外的暂停呢？比如在`lease.isValid()`那里暂停了15s，租约过期，另一个节点已经获得租约，后续代码也不会注意到租约过期，然后继续执行。

那么什么时候会出现这么长时间的暂停呢？

- *GC*，Java 的 GC 通常会时不时地暂停活动的进程；
- *虚拟机环境*，在虚拟机迁移的时候通常需要暂停虚拟机；
- *休眠*，笔记本电脑休眠；
- *线程切换*，机器负载很高的话，那么可能下次轮到这个进程执行是很晚的时候；
- *同步磁盘*，同步磁盘会引起线程挂起，虽然有些代码没有明显的同步磁盘操作，但是依然可能有意外的引入磁盘IO的操作，比如 Java 的类加载器第一次使用类文件会推迟加载，IO暂停和GC暂停可能同时发生，进一步恶化；
- *交换区*，使用磁盘交换区来虚拟内存，那么内存访问会引起缺页中断然后读磁盘；
- *SIGSTOP信号*，这个信号挂起进程。

这个问题类似于在单机系统中的多线程代码保证并发安全，但是单机系统共享内存，可以使用锁、信号量、原子计数器、无锁数据结构、阻塞队列来实现，但是分布式系统无法使用。

所以分布式的系统每个节点必须假定，进程执行过程中的任何时刻都有可能被暂停很长一段时间。

**响应时间保证**
硬件实时系统可以在硬件层面提供响应时间的保证，可以满足规定的时间约束，比如如果汽车的传感器检测到碰撞，那么肯定不希望进程因为GC而暂停。

如何保证实时？

- 首先是个**实时操作系统**，保证在规定的时间片完成调度；
- 库函数必须考虑最坏的执行时间；
- 动态分配内存受限或者完全被禁止；
- 需要大量的验证。

开发实时系统比较昂贵。

**调整垃圾回收的影响**
可以跟踪 GC，然后控制垃圾回收，把 GC 作为一个节点计划内的离线操作，节点 GC 时通知其他节点进行处理请求。

这种方法的一个变种是只对短期对象执行垃圾回收，然后在其变为长期存活对象之前，采取定期重启的策略避免对长期存活对象的全面回收。

## **知识、真相、谎言**

### **真相由多数决定**

节点无法根据自身信息来判断自身的状态，所以分布式系统不能依赖单个节点。目前许多分布式算法都依靠法定票数，即在节点间进行投票。任何决策都需要来自多个节点的最小投票数，从而减少对于特定节点的依赖。

最常见的法定票数是系统节点半数以上，如果某些节点发生故障，那么 quorum 机制可以给使系统继续工作。

**主节点和锁**
很多情况下，我们需要在系统范围内只能有一个实例，例如：

- 只允许一个节点作为数据库分区的主节点，以防止出现脑裂；
- 只允许一个事务或客户端持有特定的锁，防止同时写入；
- 只允许一个用户使用特定的用户名，从而确保用户名可以唯一标识用户。

需要注意的是即使一个节点自认为自己的是“唯一的哪一个”（主节点、锁、用户名），但不一定是获得了法定票数的同意，有可能它以前是主节点，但是被其他节点宣布失效了（比如进程暂停）。

如下图是 HBase 遇到过的问题，它的设计目标是确保存储系统的文件一次只能由一个客户端访问，如果客户端同时试图写入文件，文件就会被破坏。所以客户端写入的时候需要从锁服务中获取访问租约。

![分布式锁的不正确实现](https://raw.githubusercontent.com/qinggniq/Note/master/GO/MIT6.824/imags/DDIA/8-Trasaction/分布式锁的不正确实现.png)

这个问题属于前面说的“进程暂停”的情况，持有租约被暂停太久而到期。

**Fencing令牌**
使用锁和租约机制保证资源的并发访问的时候需要保证过期的旧节点不能影响其它正常部分，可以通过*fencing*（栅栏）来实现。

假设每次获取租约或锁的时候锁服务会返回一个*fencing令牌*，令牌计数每次被分配都会递增，然后客户端每次向存储发送写请求的时候会附加上所持有的*fencing令牌*。这样就可以防止因为进程暂停带来的问题了。

![使用fencing令牌](https://raw.githubusercontent.com/qinggniq/Note/master/GO/MIT6.824/imags/DDIA/8-Trasaction/使用fencing令牌.png)

Zookeeper 可以使用事务标识 `zxid` 或节点版本 `cversion` 来充当 *fencing令牌*。

只靠客户端检查状态是不够的，需要资源来进行主动检查，如果资源不支持额外的令牌检查，可以通过文件名内嵌令牌信息的方式来检查。

服务器不能假定客户端都是正常的。

### **拜占庭故障**

*fencing令牌*可以检测并阻止那些无意的误操作，但是如果一个节点故意破坏系统，在消息的时候伪造令牌又如何呢？

本书总是假设节点虽然不可靠但是一定是诚实的：

> 它们尽管运行很慢或者由于故障而无法响应，或者状态已经过期，但一旦响应，则一定是完全基于其所知的全部信息和实现协议好的行为准则，响应代表了其所知的“真相”。

如果节点存在撒谎的情况，那么分布式系统的处理的难度就上了一个台阶。这种行为被称为**拜占庭错误**。

如果在某个系统中即使发生部分节点故障，甚至不遵从协议，或者恶意攻击、干扰网络，仍然可以正常运行，那么我们称之为**拜占庭式容错系统**。

- 在航空航天领域，辐射可能造成内存或 CPU 的故障，以不可预知的方式响应其他节点，系统下线十分昂贵，所以需要飞机控制系统容忍拜占庭式错误；
- 比特币。

Web里面可能有恶意客户端行为，这时需要安全机制来保证，软件的 bug 可以被认为是拜占庭错误，但是如果将软件部署到所有节点上，那么即使拜占庭式错误也无法解决问题。大多数拜占庭容错算法要求系统超过三分之二的节点要功能正常。

传统的安全措施如认证、访问控制、加密、防火墙等仍然是防范攻击的主要保护措施。

**弱的谎言形式**
尽管假设节点是诚实的，但是还是需要采取一些措施来防范那些不是那么恶意的“谎言”，比如硬件造成的无效信息、软件BUG、配置错误。

- 由于硬件、操作系统、驱动程序、路由器造成的错误导致网络数据包有时出现损坏，通常可以借助 TCP/UDP 中内置的数据包校验和来发现这些问题，有时无法奏效，可以通过应用层的校验和来防范；
- 公众开放的接口必须进行参数检查；
- NTP 最好配置多个时间服务器，使得多个服务器就一定时间范围达成一致。

### **理论系统模型与现实**

目前分布式系统方面已经有很多不错的算法，这些算法要容忍本章讨论的各种故障。

算法的实现不能过分依赖于特定的硬件和软件配置，这需要我们对预习的系统错误进行形式化描述，我们通过定义一些系统模型来形式化描述算法的前提条件。

关于计时方面，有常见的三种模型：

*同步模型*
同步模型假设有上界的网络延时，有上界的进程暂停和有上界的时钟误差，大多数系统并非同步模型，因为无限延迟和暂停确实可能发生。

*部分同步模型*
意味着系统绝大多数情况下像一个同步系统运行，但是有时会出现超出网络延迟，进程暂停和时钟漂移的预期上界。

*异步模型*
算法不会对时机做任何假设，甚至里面没有时钟（也没有超时机制）。

关于节点时效方面，有常见的三种模型：

*崩溃-中止模型*
此模型下节点只会遭遇一种故障，即遭遇系统崩溃。节点突然失去响应，然后永远消失无法恢复。

*崩溃-恢复模型*
节点可能在任何时候发生崩溃，且会在一段（未知）时间之后得到恢复并再次响应，节点中的持久性存储数据在崩溃中得以保存，而内存状态可能消失。

*拜占庭失效模型*
节点可能发生任何事情，包括作弊和欺骗其他节点。

**算法的正确性**
定义算法的正确性，我们可以描述它的属性信息。比如之前的*fencing令牌*生成算法要求算法具有一下属性：

- *唯一性*：两个令牌不能获得一样的值
- *单调递增*：如果请求$x$返回了令牌$t{x}$，请求$y$返回了$t{y}$，$x$在$y$之前完成，那么$t{x} < t{y}$
- *可用性*：请求令牌的节点如果不发生崩溃则一定会收到响应

如果针对某个系统模型的算法在各种情况下都能满足定义好的属性请求，那么我们称这个算法是正确的。

**安全性和活性**
在上面的例子中，唯一性和单调递增性属于安全性，而可用性属于活性。

> 活性的定义中一般包括暗示“最终”一词。

安全性可以理解为**没有发生意外**，活性可以理解为**预期的事最终一定会发生**，具体的定义如下：

- 如果违反了安全属性，我们可以明确的执行发生的时间点，一旦违反，违规行为无法撤销，破坏已经实际发生；
- 活性则反过来：可能无法明确一个时间点，但总是希望未来某个时间点可以满足要求。

区分安全性和活性好处是可以简化处理一些具有挑战性的系统模型，对于分布式算法，要求所有可能的系统模型下，都必须符合安全属性，也就是说如果节点崩溃，网络中断，算法确保不会返回错误的结果。

对于活性，则存在一些必要条件才能满足。比如只有在大多数节点没有崩溃，以及网络最终可以恢复的前提下，我们才能最终收到响应。
